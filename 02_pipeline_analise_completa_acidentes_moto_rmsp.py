import calendar
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, classification_report
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler

from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler

from scipy import stats

plt.style.use('seaborn-v0_8')
plt.rcParams['figure.figsize'] = (14, 8)
plt.rcParams['font.size'] = 10


print("AN√ÅLISE COMPLETA: ACIDENTES DE MOTOCICLETA NA RMSP")


# PARTE I: TRATAMENTO ROBUSTO DE DADOS


# Carregar dados

df = pd.read_csv("pessoas_2020-2025.csv", encoding="latin-1", sep=";")

# Remover linhas duplicadas
df = df.drop_duplicates()

# Remover colunas duplicadas (caso existam)
df = df.loc[:, ~df.columns.duplicated()]

# Padronizar valores faltantes e inconsistentes

# Tratar faltantes corretamente por tipo
for col in df.columns:
    if df[col].dtype == 'O' or str(df[col].dtype).startswith('category'):
        df[col] = df[col].replace('NAO DISPONIVEL', 'DESCONHECIDO')
        df[col] = df[col].fillna('DESCONHECIDO')
    elif np.issubdtype(df[col].dtype, np.number):
        df[col] = df[col].fillna(df[col].median())

# Padronizar nomes de cidades (remover espa√ßos, acentos, caixa baixa)
import unicodedata
if 'municipio' in df.columns:
    df['municipio'] = df['municipio'].astype(str).str.strip().str.upper()
    df['municipio'] = df['municipio'].apply(lambda x: unicodedata.normalize('NFKD', x).encode('ASCII', 'ignore').decode('ASCII'))

# Discretizar idade em faixas
if 'idade' in df.columns:
    df['idade_faixa'] = pd.cut(df['idade'], bins=[0, 18, 25, 35, 45, 60, 100], labels=["0-18", "19-25", "26-35", "36-45", "46-60", "60+"])

# Tratamento de outliers (apenas idade)
if 'idade' in df.columns:
    Q1 = df['idade'].quantile(0.25)
    Q3 = df['idade'].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    df = df[(df['idade'] >= lower) & (df['idade'] <= upper)]

# Normaliza√ß√£o de vari√°veis num√©ricas
num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
# Remover mes_sinistro e dia_sinistro da lista de normaliza√ß√£o
for col_to_exclude in ['mes_sinistro', 'dia_sinistro']:
    if col_to_exclude in num_cols:
        num_cols.remove(col_to_exclude)
if num_cols:
    scaler = StandardScaler()
    df[num_cols] = scaler.fit_transform(df[num_cols])

# Discretizar nomes das cidades como n√∫meros (LabelEncoder)
if 'municipio' in df.columns:
    le_mun = LabelEncoder()
    df['municipio_num'] = le_mun.fit_transform(df['municipio'])

# Preencher dados faltantes gen√©ricos

# Preencher dados faltantes apenas para colunas num√©ricas
for col in df.select_dtypes(include=[np.number]).columns:
    df[col] = df[col].fillna(df[col].median())

# Discretizar idade em faixas
if 'idade' in df.columns:
    df['idade_faixa'] = pd.cut(df['idade'], bins=[0, 18, 25, 35, 45, 60, 100], labels=["0-18", "19-25", "26-35", "36-45", "46-60", "60+"])

# Tratamento de outliers (apenas idade)
if 'idade' in df.columns:
    Q1 = df['idade'].quantile(0.25)
    Q3 = df['idade'].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    df = df[(df['idade'] >= lower) & (df['idade'] <= upper)]

# FILTRO AP√ìS TRATAMENTO
df = df[(df["tipo_veiculo_vitima"] == "MOTOCICLETA") &
    (df["regiao_administrativa"] == "METROPOLITANA DE S√ÉO PAULO")]
df = df[df["gravidade_lesao"] != "DESCONHECIDO"]
df = df[df["faixa_etaria_legal"] != "DESCONHECIDO"]
df = df[df["sexo"] != "DESCONHECIDO"]

print(f"Base final: {df.shape[0]:,} registros")


# PARTE II: AN√ÅLISE EXPLORAT√ìRIA


print("\nAN√ÅLISE EXPLORAT√ìRIA")

# 2.1 Histograma de idades
plt.figure()
sns.histplot(df["idade"].dropna(), bins=30, kde=True, color="steelblue")
plt.title("Distribui√ß√£o da Idade")
plt.xlabel("Idade")
plt.ylabel("Frequ√™ncia")
plt.savefig("output/02_pipeline_ml/histograma_idade.png")

# 2.2 Boxplot idade vs gravidade (detec√ß√£o de outliers)
plt.figure()
sns.boxplot(x="gravidade_lesao", y="idade", data=df, hue="gravidade_lesao", palette="Set2", legend=False)
plt.title("Boxplot da Idade por Gravidade da Les√£o")
plt.savefig("output/02_pipeline_ml/boxplot_idade_gravidade.png")

# 2.3 Desvio padr√£o da idade por gravidade
desvios = df.groupby("gravidade_lesao")["idade"].std().round(2)
print("\nDesvio padr√£o da idade por gravidade:")
print(desvios)

# 2.4 Correla√ß√£o idade √ó gravidade (codificando gravidade em n√∫meros)
grav_encoder = LabelEncoder()
df["gravidade_num"] = grav_encoder.fit_transform(df["gravidade_lesao"])

corr_val = df[["idade", "gravidade_num"]].corr().iloc[0, 1]
print(f"\nCorrela√ß√£o idade √ó gravidade: {corr_val:.3f}")

plt.figure()
sns.regplot(x="idade", y="gravidade_num", data=df, logistic=True, ci=None, scatter_kws={'alpha':0.2})
plt.title("Correla√ß√£o Idade √ó Gravidade (logit)")
plt.savefig("output/02_pipeline_ml/correlacao_idade_gravidade.png")


# 2.5: AN√ÅLISE DE CORRELA√á√ïES

print("\nAN√ÅLISE DE CORRELA√á√ïES COM GRAVIDADE")

# Criar c√≥pia para an√°lise de correla√ß√µes
df_corr = df.copy()

# Codificar todas as vari√°veis categ√≥ricas para an√°lise de correla√ß√£o
categorical_cols = df_corr.select_dtypes(include=['object']).columns.tolist()
categorical_cols = [col for col in categorical_cols if col != 'gravidade_lesao']  # Excluir target

encoders = {}
for col in categorical_cols:
    if df_corr[col].notna().sum() > 0:  # Apenas se h√° dados v√°lidos
        le = LabelEncoder()
        # Tratar valores ausentes
        df_corr[col] = df_corr[col].fillna('MISSING')
        df_corr[col] = le.fit_transform(df_corr[col].astype(str))
        encoders[col] = le

# Selecionar apenas colunas num√©ricas para correla√ß√£o
numeric_cols = df_corr.select_dtypes(include=[np.number]).columns.tolist()
if 'gravidade_num' in numeric_cols:
    numeric_cols.remove('gravidade_num')  # Para calcular separadamente

# Calcular correla√ß√µes com gravidade
correlations = {}
for col in numeric_cols:
    if col in df_corr.columns and df_corr[col].notna().sum() > 1:
        corr = df_corr[col].corr(df_corr['gravidade_num'])
        if not np.isnan(corr):
            correlations[col] = abs(corr)  # Valor absoluto para ordenar por for√ßa

# Ordenar por correla√ß√£o (mais forte primeiro)
correlations_sorted = dict(sorted(correlations.items(), key=lambda x: x[1], reverse=True))

print("\nRANKING DE CORRELA√á√ïES (por for√ßa):")
print("=" * 50)
for i, (col, corr_abs) in enumerate(correlations_sorted.items(), 1):
    # Pegar correla√ß√£o original (com sinal)
    original_corr = df_corr[col].corr(df_corr['gravidade_num'])
    print(f"{i:2d}. {col:<25} = {original_corr:+.4f} (|{corr_abs:.4f}|)")

# Visualizar top 5 correla√ß√µes
top_5_cols = list(correlations_sorted.keys())[:5]
if len(top_5_cols) > 0:
    plt.figure(figsize=(15, 10))
    for i, col in enumerate(top_5_cols, 1):
        plt.subplot(2, 3, i)
        original_corr = df_corr[col].corr(df_corr['gravidade_num'])
        
        # Scatter plot com regress√£o
        sns.regplot(data=df_corr, x=col, y='gravidade_num', 
                   scatter_kws={'alpha': 0.3}, line_kws={'color': 'red'})
        plt.title(f'{col}\nCorr = {original_corr:+.4f}')
        plt.xlabel(col)
        plt.ylabel('Gravidade (num√©rica)')
    
    plt.tight_layout()
    plt.savefig('output/02_pipeline_ml/top_5_correlacoes.png', dpi=300, bbox_inches='tight')

# Criar heatmap de correla√ß√µes (se h√° pelo menos 3 vari√°veis)
if len(correlations_sorted) >= 3:
    # Pegar top vari√°veis para heatmap
    top_vars = ['gravidade_num'] + list(correlations_sorted.keys())[:10]
    corr_matrix = df_corr[top_vars].corr()
    
    plt.figure(figsize=(12, 8))
    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, square=True, fmt='.3f',
            cbar_kws={'shrink': 0.8, 'label': 'Correla√ß√£o'}, linewidths=1.0,
            annot_kws={'fontsize': 9, 'fontweight': 'bold'}, vmin=-1, vmax=1)
    plt.title('Matriz de Correla√ß√£o - Top Vari√°veis')
    plt.tight_layout()
    plt.savefig('output/02_pipeline_ml/matriz_correlacao.png', dpi=300, bbox_inches='tight')

print(f"\nVARI√ÅVEL COM MAIOR CORRELA√á√ÉO: {list(correlations_sorted.keys())[0] if correlations_sorted else 'Nenhuma encontrada'}")
if correlations_sorted:
    best_col = list(correlations_sorted.keys())[0]
    best_corr = df_corr[best_col].corr(df_corr['gravidade_num'])
    print(f"   Correla√ß√£o: {best_corr:+.4f}")
    
    # An√°lise descritiva da melhor vari√°vel
    print(f"\nAN√ÅLISE DESCRITIVA - {best_col}:")
    print("=" * 50)
    grouped = df_corr.groupby('gravidade_lesao')[best_col].describe()
    print(grouped.round(3))

# 2.6: AN√ÅLISE PREDITIVA (SEM DADOS DE √ìBITO)

print("\nAN√ÅLISE DE VARI√ÅVEIS PREDITIVAS (dispon√≠veis ANTES do acidente)")

# Criar dataset apenas com vari√°veis preditivas (excluindo √≥bito)
df_pred = df.copy()

# Excluir todas as vari√°veis relacionadas a √≥bito e informa√ß√µes n√£o dispon√≠veis na hora do acidente
exclude_cols = ["local_obito", "tempo_sinistro_obito", "ano_obito", "dia_obito", 
                "mes_obito", "ano_mes_obito", "data_obito", "nacionalidade",
                "grau_de_instrucao", "profissao", "id_sinistro", "id_veiculo",
                "cod_ibge", "regiao_administrativa", "regiao_norm", "tipo_veiculo_vitima", "gravidade_num"]

# Manter apenas vari√°veis preditivas
predictive_vars = [col for col in df_pred.columns if col not in exclude_cols]
df_pred = df_pred[predictive_vars]

print(f"Vari√°veis preditivas analisadas: {[col for col in predictive_vars if col != 'gravidade_lesao']}")

# Codificar vari√°veis categ√≥ricas para correla√ß√£o
df_pred_corr = df_pred.copy()
categorical_cols_pred = df_pred_corr.select_dtypes(include=['object']).columns.tolist()
categorical_cols_pred = [col for col in categorical_cols_pred if col != 'gravidade_lesao']

for col in categorical_cols_pred:
    if df_pred_corr[col].notna().sum() > 0:
        le = LabelEncoder()
        df_pred_corr[col] = df_pred_corr[col].fillna('MISSING')
        df_pred_corr[col] = le.fit_transform(df_pred_corr[col].astype(str))

# Correla√ß√µes apenas com vari√°veis preditivas
grav_encoder_pred = LabelEncoder()
df_pred_corr["gravidade_num"] = grav_encoder_pred.fit_transform(df_pred_corr["gravidade_lesao"])

numeric_cols_pred = df_pred_corr.select_dtypes(include=[np.number]).columns.tolist()
numeric_cols_pred = [col for col in numeric_cols_pred if col != 'gravidade_num']

correlations_pred = {}
for col in numeric_cols_pred:
    if col in df_pred_corr.columns and df_pred_corr[col].notna().sum() > 1:
        corr = df_pred_corr[col].corr(df_pred_corr['gravidade_num'])
        if not np.isnan(corr):
            correlations_pred[col] = abs(corr)

correlations_pred_sorted = dict(sorted(correlations_pred.items(), key=lambda x: x[1], reverse=True))

print("\nRANKING VARI√ÅVEIS PREDITIVAS (por correla√ß√£o com gravidade):")
print("=" * 60)
for i, (col, corr_abs) in enumerate(correlations_pred_sorted.items(), 1):
    original_corr = df_pred_corr[col].corr(df_pred_corr['gravidade_num'])
    print(f"{i:2d}. {col:<25} = {original_corr:+.4f} (|{corr_abs:.4f}|)")

# An√°lise cruzada das top vari√°veis preditivas
if len(correlations_pred_sorted) >= 2:
    top_predictive = list(correlations_pred_sorted.keys())[:3]  # Top 3
    
    print(f"\nAN√ÅLISE CRUZADA - TOP VARI√ÅVEIS PREDITIVAS:")
    print("=" * 60)
    
    for var in top_predictive:
        print(f"\nüî∏ {var.upper()}:")
        if var in df_pred.columns:
            crosstab = pd.crosstab(df_pred[var], df_pred['gravidade_lesao'], normalize='index')
            print(crosstab.round(3))
    
    # Visualiza√ß√£o das top vari√°veis preditivas
    plt.figure(figsize=(16, 12))
    for i, col in enumerate(top_predictive, 1):
        plt.subplot(2, 3, i)
        original_corr = df_pred_corr[col].corr(df_pred_corr['gravidade_num'])
        
        sns.regplot(data=df_pred_corr, x=col, y='gravidade_num', 
                   scatter_kws={'alpha': 0.3}, line_kws={'color': 'red'})
        plt.title(f'{col} (Preditiva)\nCorrela√ß√£o = {original_corr:+.4f}')
        plt.xlabel(col)
        plt.ylabel('Gravidade')
    
    plt.suptitle('TOP VARI√ÅVEIS PREDITIVAS vs GRAVIDADE', fontsize=14, y=0.98)
    plt.tight_layout()
    plt.savefig('output/02_pipeline_ml/variaveis_preditivas.png', dpi=300, bbox_inches='tight')
    
    # An√°lise estat√≠stica das diferen√ßas
    print(f"\nSIGNIFIC√ÇNCIA DAS VARI√ÅVEIS PREDITIVAS:")
    print("=" * 60)
    
    for var in top_predictive:
        if var in df_pred_corr.columns:
            groups = []
            for grav in df_pred['gravidade_lesao'].unique():
                group_data = df_pred_corr[df_pred['gravidade_lesao'] == grav][var].dropna()
                if len(group_data) > 0:
                    groups.append(group_data)
            
            if len(groups) >= 2:
                try:
                    if len(groups) == 2:
                        stat, p_value = stats.ttest_ind(groups[0], groups[1])
                        test_name = "T-test"
                    else:
                        stat, p_value = stats.f_oneway(*groups)
                        test_name = "ANOVA"
                    
                    significance = "***" if p_value < 0.001 else "**" if p_value < 0.01 else "*" if p_value < 0.05 else "ns"
                    print(f"{var:<25}: p-value = {p_value:.6f} {significance} ({test_name})")
                except:
                    print(f"{var:<25}: Erro no teste estat√≠stico")


# PARTE III: PREPARA√á√ÉO PARA MACHINE LEARNING (APENAS VARI√ÅVEIS PREDITIVAS)

print("\nPREPARA√á√ÉO PARA MACHINE LEARNING (VARI√ÅVEIS PREDITIVAS)")

df_ml = df_pred.copy()

# Agora removemos apenas as vari√°veis definitivamente desnecess√°rias para ML
drop_cols_ml = ["data_sinistro", "ano_mes_sinistro"]  # Datas espec√≠ficas n√£o s√£o √∫teis para ML

df_ml = df_ml.drop(columns=[c for c in drop_cols_ml if c in df_ml.columns])

print(f"Registros antes da limpeza: {len(df_ml):,}")
print(f"Colunas restantes: {df_ml.columns.tolist()}")

# Remover apenas linhas com valores ausentes nas colunas essenciais
essential_cols = ['sexo', 'gravidade_lesao', 'faixa_etaria_legal', 'tipo_de_vitima']
for col in essential_cols:
    if col in df_ml.columns:
        before = len(df_ml)
        df_ml = df_ml[df_ml[col].notna() & (df_ml[col] != 'NAO DISPONIVEL')]
        after = len(df_ml)
        print(f"Ap√≥s limpar {col}: {after:,} registros (removidos: {before-after:,})")

# Sexo ‚Üí bin√°rio
df_ml["sexo"] = df_ml["sexo"].map({"FEMININO": 0, "MASCULINO": 1})

# Codifica√ß√£o de categ√≥ricas com OneHotEncoder
cat_cols = ["municipio", "tipo_via", "tipo_de_vitima", "faixa_etaria_legal", "faixa_etaria_demografica"]
ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
cat_data = df_ml[cat_cols].astype(str)
cat_encoded = ohe.fit_transform(cat_data)
cat_feature_names = ohe.get_feature_names_out(cat_cols)
cat_encoded_df = pd.DataFrame(cat_encoded, columns=cat_feature_names, index=df_ml.index)
df_ml = pd.concat([df_ml.drop(columns=cat_cols), cat_encoded_df], axis=1)
print(f"OneHotEncoder aplicado nas colunas: {cat_cols}")

# Target

y = grav_encoder_pred.fit_transform(df_ml["gravidade_lesao"])
X = df_ml.drop(columns=["gravidade_lesao"])
# Remover colunas categ√≥ricas (category) de X
cat_cols_to_remove = [col for col in X.columns if str(X[col].dtype).startswith('category')]
if cat_cols_to_remove:
    X = X.drop(columns=cat_cols_to_remove)
# Garantir que n√£o h√° valores ausentes restantes nas colunas num√©ricas
for col in X.columns:
    if np.issubdtype(X[col].dtype, np.number):
        X[col] = X[col].fillna(0)

print(f"Dataset final: {X.shape[0]:,} amostras, {X.shape[1]} features")
print(f"Features utilizadas: {X.columns.tolist()}")
print(f"Distribui√ß√£o das classes:")
unique, counts = np.unique(y, return_counts=True)
for i, count in zip(unique, counts):
    class_name = grav_encoder_pred.inverse_transform([i])[0]
    print(f"  {class_name}: {count:,} ({count/len(y)*100:.1f}%)")

# Import√¢ncia das features no modelo
print(f"\nIMPORT√ÇNCIA DAS FEATURES (Random Forest):")
print("=" * 50)

# Treinar modelo tempor√°rio para ver import√¢ncia das features
temp_X_train, temp_X_test, temp_y_train, temp_y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
temp_clf = RandomForestClassifier(n_estimators=100, random_state=42)
temp_clf.fit(temp_X_train, temp_y_train)

# Ordenar features por import√¢ncia
feature_importance = list(zip(X.columns, temp_clf.feature_importances_))
feature_importance.sort(key=lambda x: x[1], reverse=True)

for i, (feature, importance) in enumerate(feature_importance, 1):
    print(f"{i:2d}. {feature:<25} = {importance:.4f}")

# Visualizar import√¢ncia das features
plt.figure(figsize=(10, 6))
# Selecionar apenas as 15 features mais importantes
top_n = 15
top_features = feature_importance[:top_n]
features, importances = zip(*top_features)
plt.barh(range(len(features)), importances)
plt.yticks(range(len(features)), features)
plt.xlabel('Import√¢ncia')
plt.title(f'Top {top_n} Features Mais Importantes (Random Forest)')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.savefig('output/02_pipeline_ml/importancia_features.png', dpi=300, bbox_inches='tight')


# PARTE IV: TREINO SEM BALANCEAMENTO


print("\nTREINAMENTO SEM BALANCEAMENTO (APENAS VARI√ÅVEIS PREDITIVAS)")

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
# Usar class_weight='balanced' para todos os modelos
clf_baseline = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
clf_baseline.fit(X_train, y_train)
y_pred_base = clf_baseline.predict(X_test)

f1_base = f1_score(y_test, y_pred_base, average="macro")
print(f"F1-score (baseline): {f1_base:.4f}")

# Relat√≥rio detalhado
print("\nRELAT√ìRIO DETALHADO (Baseline):")
print(classification_report(y_test, y_pred_base, target_names=grav_encoder_pred.classes_))

# Avalia√ß√£o focada em FATAL e GRAVE
labels_fatal_grave = [list(grav_encoder_pred.classes_).index('FATAL'), list(grav_encoder_pred.classes_).index('GRAVE')]
precision_fg = precision_score(y_test, y_pred_base, labels=labels_fatal_grave, average=None)
recall_fg = recall_score(y_test, y_pred_base, labels=labels_fatal_grave, average=None)
f1_fg = f1_score(y_test, y_pred_base, labels=labels_fatal_grave, average=None)
import pandas as pd
df_fg = pd.DataFrame({
    'Classe': ['FATAL', 'GRAVE'],
    'Precision': precision_fg,
    'Recall': recall_fg,
    'F1': f1_fg
})
df_fg.to_csv('output/02_pipeline_ml/metricas_fatal_grave.csv', index=False, float_format='%.4f', encoding='utf-8')
print("\nM√©tricas focadas em FATAL e GRAVE salvas em output/02_pipeline_ml/metricas_fatal_grave.csv:")
print(df_fg)

# Threshold customizado para maximizar recall de FATAL/GRAVE
print("\nRELAT√ìRIO DETALHADO (Baseline - Threshold customizado):")
proba_base = clf_baseline.predict_proba(X_test)
# Definir threshold menor para FATAL (classe 2) e GRAVE (classe 1)
threshold_fatal = 0.3
threshold_grave = 0.3
y_pred_thresh = []
for p in proba_base:
    if p[2] >= threshold_fatal:
        y_pred_thresh.append(2)
    elif p[1] >= threshold_grave:
        y_pred_thresh.append(1)
    else:
        y_pred_thresh.append(0)
print(classification_report(y_test, y_pred_thresh, target_names=grav_encoder_pred.classes_))

# Avalia√ß√£o focada em FATAL e GRAVE para threshold customizado
precision_fg_t = precision_score(y_test, y_pred_thresh, labels=labels_fatal_grave, average=None)
recall_fg_t = recall_score(y_test, y_pred_thresh, labels=labels_fatal_grave, average=None)
f1_fg_t = f1_score(y_test, y_pred_thresh, labels=labels_fatal_grave, average=None)
df_fg_t = pd.DataFrame({
    'Classe': ['FATAL', 'GRAVE'],
    'Precision': precision_fg_t,
    'Recall': recall_fg_t,
    'F1': f1_fg_t
})
df_fg_t.to_csv('output/02_pipeline_ml/metricas_fatal_grave_threshold.csv', index=False, float_format='%.4f', encoding='utf-8')
print("\nM√©tricas focadas em FATAL e GRAVE (threshold customizado) salvas em output/02_pipeline_ml/metricas_fatal_grave_threshold.csv:")
print(df_fg_t)


# PARTE V.1: TREINO COM SMOTE

print("\nTREINAMENTO COM SMOTE (APENAS VARI√ÅVEIS PREDITIVAS)")

smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X_train, y_train)

clf_smote = RandomForestClassifier(n_estimators=100, random_state=42, class_weight="balanced")
clf_smote.fit(X_res, y_res)
y_pred_smote = clf_smote.predict(X_test)

f1_smote = f1_score(y_test, y_pred_smote, average="macro")
print(f"F1-score (SMOTE): {f1_smote:.4f}")

# Relat√≥rio detalhado SMOTE
print("\nRELAT√ìRIO DETALHADO (SMOTE):")
print(classification_report(y_test, y_pred_smote, target_names=grav_encoder_pred.classes_))


# PARTE V.2: TREINO COM RANDOM UNDERSAMPLING

print("\nTREINAMENTO COM RANDOM UNDERSAMPLING (APENAS VARI√ÅVEIS PREDITIVAS)")

rus = RandomUnderSampler(random_state=42)
X_rus, y_rus = rus.fit_resample(X_train, y_train)

clf_rus = RandomForestClassifier(n_estimators=100, random_state=42, class_weight="balanced")
clf_rus.fit(X_rus, y_rus)
y_pred_rus = clf_rus.predict(X_test)

f1_rus = f1_score(y_test, y_pred_rus, average="macro")
print(f"F1-score (RandomUnderSampler): {f1_rus:.4f}")

# Relat√≥rio detalhado RandomUnderSampler
print("\nRELAT√ìRIO DETALHADO (RandomUnderSampler):")
print(classification_report(y_test, y_pred_rus, target_names=grav_encoder_pred.classes_))


# PARTE V.3: OTIMIZA√á√ÉO DE HIPERPAR√ÇMETROS COM GRIDSEARCHCV

print("\nOtimizando hiperpar√¢metros com GridSearchCV (Random Forest)...")

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 5, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

rf = RandomForestClassifier(random_state=42, class_weight="balanced")
gs = GridSearchCV(rf, param_grid, cv=3, n_jobs=-1, verbose=1)
gs.fit(X_res, y_res)

print(f"Melhores par√¢metros encontrados: {gs.best_params_}")
print(f"Melhor score de valida√ß√£o cruzada: {gs.best_score_:.3f}")

# Avalia√ß√£o no teste
y_pred_gs = gs.predict(X_test)
print("\nRelat√≥rio de classifica√ß√£o (GridSearchCV - teste):")
print(classification_report(y_test, y_pred_gs, target_names=grav_encoder_pred.classes_))

# Salvar resultados em arquivo txt
with open('output/02_pipeline_ml/GRIDSEARCHCV_RESULTADOS.txt', 'w', encoding='utf-8') as f:
    f.write(f"Melhores par√¢metros: {gs.best_params_}\n")
    f.write(f"Melhor score de valida√ß√£o cruzada: {gs.best_score_:.3f}\n\n")
    f.write("Relat√≥rio de classifica√ß√£o (teste):\n")
    f.write(classification_report(y_test, y_pred_gs, target_names=grav_encoder_pred.classes_))
print("Resultados do GridSearchCV salvos em output/02_pipeline_ml/GRIDSEARCHCV_RESULTADOS.txt")


# PARTE VI: COMPARA√á√ÉO E CONCLUS√ïES

print("\nCOMPARA√á√ÉO FINAL - MODELO PREDITIVO")
print("=" * 50)

print(f"   ‚Ä¢ Sem balanceamento: {f1_base:.4f}")
print(f"   ‚Ä¢ Com SMOTE:         {f1_smote:.4f}")
print(f"   ‚Ä¢ Com RandomUnderSampler: {f1_rus:.4f}")


if f1_smote > f1_base and f1_smote > f1_rus:
    melhor_modelo = "SMOTE"
    melhor_score = f1_smote
elif f1_rus > f1_base and f1_rus > f1_smote:
    melhor_modelo = "RandomUnderSampler"
    melhor_score = f1_rus
else:
    melhor_modelo = "Baseline"
    melhor_score = f1_base

print(f"\nMELHOR MODELO: {melhor_modelo} (F1-score: {melhor_score:.4f})")

print(f"\nRESUMO:")
print("=" * 50)
print(f"‚Ä¢ Total de registros analisados: {len(df):,}")
print(f"‚Ä¢ Registros para modelagem: {len(X):,}")
print(f"‚Ä¢ Vari√°veis preditivas utilizadas: {len(X.columns)}")
print(f"‚Ä¢ Melhor modelo: {melhor_modelo}")
print(f"‚Ä¢ Performance (F1-macro): {melhor_score:.4f}")

if correlations_pred_sorted:
    top_3_vars = list(correlations_pred_sorted.keys())[:3]
    print(f"‚Ä¢ Top 3 vari√°veis preditivas:")
    for i, var in enumerate(top_3_vars, 1):
        corr_val = df_pred_corr[var].corr(df_pred_corr['gravidade_num'])
        print(f"  {i}. {var} (correla√ß√£o: {corr_val:+.4f})")

print("\nAN√ÅLISE PREDITIVA COMPLETA!")


# PARTE VII: AN√ÅLISE TEMPORAL DETALHADA

print("\nAN√ÅLISE TEMPORAL DETALHADA")
print("=" * 60)

# An√°lise por m√™s
print("\nPADR√ïES MENSAIS:")
monthly_analysis = df_pred.groupby(['mes_sinistro', 'gravidade_lesao']).size().unstack(fill_value=0)
monthly_pct = monthly_analysis.div(monthly_analysis.sum(axis=1), axis=0)

print("\nDistribui√ß√£o por m√™s (%):")
print(monthly_pct.round(3))

# Identificar meses mais perigosos
monthly_fatal = monthly_pct['FATAL'].sort_values(ascending=False)
print(f"\nMESES MAIS FATAIS:")
for i, (mes, pct) in enumerate(monthly_fatal.head(3).items(), 1):
    print(f"{i}. M√™s {mes}: {pct:.3f} ({pct*100:.1f}% de acidentes fatais)")

# An√°lise por dia do m√™s
print("\nPADR√ïES POR DIA DO M√äS:")
# Usar valores originais para dia
df_temporal = df_pred.copy()
if 'dia_sinistro' in df_temporal.columns:
    df_temporal['dia_sinistro_orig'] = df_temporal['dia_sinistro'].astype(int)
    daily_analysis = df_temporal.groupby(['dia_sinistro_orig', 'gravidade_lesao']).size().unstack(fill_value=0)
    daily_pct = daily_analysis.div(daily_analysis.sum(axis=1), axis=0)

    # Top 5 dias mais perigosos
    daily_fatal = daily_pct['FATAL'].sort_values(ascending=False)
    print(f"\nDIAS DO M√äS MAIS FATAIS:")
    for i, (dia, pct) in enumerate(daily_fatal.head(5).items(), 1):
        print(f"{i}. Dia {int(dia)}: {pct:.3f} ({pct*100:.1f}% de acidentes fatais)")

# Visualiza√ß√£o temporal
plt.figure(figsize=(16, 10))

# Subplot 1: Padr√£o mensal
plt.subplot(2, 2, 1)
monthly_pct.plot(kind='bar', stacked=False, ax=plt.gca())
plt.title('Distribui√ß√£o de Gravidade por M√™s')
plt.xlabel('M√™s')
plt.ylabel('Propor√ß√£o')
plt.xticks(rotation=0)
plt.legend(title='Gravidade')

# Subplot 2: Padr√£o di√°rio (apenas fatais)
plt.subplot(2, 2, 2)
daily_fatal.plot(kind='line', marker='o', color='red')
plt.title('Propor√ß√£o de Acidentes Fatais por Dia do M√™s')
plt.xlabel('Dia do M√™s')
plt.ylabel('Propor√ß√£o Fatal')
plt.grid(True, alpha=0.3)

# Subplot 3: Heatmap temporal
plt.subplot(2, 2, 3)

# Heatmap temporal com r√≥tulos corrigidos

# Usar os valores originais de mes_sinistro e dia_sinistro para o heatmap
df_temporal = df_pred.copy()
if 'mes_sinistro' in df_temporal.columns and 'dia_sinistro' in df_temporal.columns:
    # Garantir que n√£o est√£o normalizados
    df_temporal['mes_sinistro_orig'] = df_temporal['mes_sinistro']
    df_temporal['dia_sinistro_orig'] = df_temporal['dia_sinistro']
    pivot_temporal = df_temporal.pivot_table(values='idade', index='mes_sinistro_orig', columns='dia_sinistro_orig', aggfunc='count', fill_value=0)
    ax = plt.gca()
    sns.heatmap(pivot_temporal, cmap='YlOrRd', cbar_kws={'label': 'N√∫mero de Acidentes'}, ax=ax)
    ax.set_title('Heatmap: Acidentes por M√™s vs Dia')
    ax.set_xlabel('Dia do M√™s')
    ax.set_ylabel('M√™s')

    # R√≥tulos dos dias (1-31)
    dias_labels = [str(int(d)) for d in pivot_temporal.columns]
    ax.set_xticks(np.arange(len(dias_labels)) + 0.5)
    ax.set_xticklabels(dias_labels, rotation=0)

    # R√≥tulos dos meses abreviados
    meses_labels = [calendar.month_abbr[int(m)].lower() if int(m) >= 1 and int(m) <= 12 else str(int(m)) for m in pivot_temporal.index]
    ax.set_yticks(np.arange(len(meses_labels)) + 0.5)
    ax.set_yticklabels(meses_labels, rotation=0)

    plt.tight_layout()
    plt.savefig('output/02_pipeline_ml/analise_temporal_detalhada.png', dpi=300, bbox_inches='tight')


# PARTE VIII: AN√ÅLISE GEOGR√ÅFICA DETALHADA

print("\nAN√ÅLISE GEOGR√ÅFICA DETALHADA")
print("=" * 60)

# Ranking completo dos munic√≠pios por risco
municipal_analysis = df_pred.groupby(['municipio', 'gravidade_lesao']).size().unstack(fill_value=0)
municipal_pct = municipal_analysis.div(municipal_analysis.sum(axis=1), axis=0)
municipal_total = municipal_analysis.sum(axis=1)

# Considerar apenas munic√≠pios com pelo menos 100 acidentes para estat√≠stica confi√°vel
municipal_filtered = municipal_pct[municipal_total >= 100].copy()
municipal_filtered['Total_Acidentes'] = municipal_total[municipal_total >= 100]

# Ordenar por taxa de fatalidade
municipal_risk = municipal_filtered.sort_values('FATAL', ascending=False)

print(f"\nRANKING DE MUNIC√çPIOS POR RISCO (min. 100 acidentes):")
print("=" * 80)
print(f"{'Pos':<3} {'Munic√≠pio':<25} {'Fatal%':<8} {'Grave%':<8} {'Leve%':<8} {'Total':<6}")
print("-" * 80)

for i, (municipio, row) in enumerate(municipal_risk.head(10).iterrows(), 1):
    fatal_pct = row['FATAL'] * 100
    grave_pct = row['GRAVE'] * 100 
    leve_pct = row['LEVE'] * 100
    total = int(row['Total_Acidentes'])
    print(f"{i:<3} {municipio:<25} {fatal_pct:<8.1f} {grave_pct:<8.1f} {leve_pct:<8.1f} {total:<6}")

print(f"\nMUNIC√çPIOS MAIS SEGUROS:")
print("=" * 80)
municipal_safe = municipal_filtered.sort_values('FATAL', ascending=True)
for i, (municipio, row) in enumerate(municipal_safe.head(5).iterrows(), 1):
    fatal_pct = row['FATAL'] * 100
    total = int(row['Total_Acidentes'])
    print(f"{i}. {municipio}: {fatal_pct:.1f}% fatais ({total} acidentes)")

# An√°lise estat√≠stica dos munic√≠pios
print(f"\nESTAT√çSTICAS MUNICIPAIS:")
print(f"‚Ä¢ M√©dia de fatalidade: {municipal_filtered['FATAL'].mean()*100:.2f}%")
print(f"‚Ä¢ Mediana de fatalidade: {municipal_filtered['FATAL'].median()*100:.2f}%")
print(f"‚Ä¢ Desvio padr√£o: {municipal_filtered['FATAL'].std()*100:.2f}%")
print(f"‚Ä¢ Munic√≠pio mais perigoso: {municipal_risk.index[0]} ({municipal_risk['FATAL'].iloc[0]*100:.1f}%)")
print(f"‚Ä¢ Munic√≠pio mais seguro: {municipal_safe.index[0]} ({municipal_safe['FATAL'].iloc[0]*100:.1f}%)")

# Visualiza√ß√£o geogr√°fica
plt.figure(figsize=(14, 8))

# Top 15 munic√≠pios por risco
top_15_risk = municipal_risk.head(15)
colors = ['red' if x > municipal_filtered['FATAL'].mean() else 'orange' for x in top_15_risk['FATAL']]

plt.barh(range(len(top_15_risk)), top_15_risk['FATAL']*100, color=colors)
plt.yticks(range(len(top_15_risk)), top_15_risk.index)
plt.xlabel('Porcentagem de Acidentes Fatais (%)')
plt.title('Top 15 Munic√≠pios com Maior Taxa de Fatalidade')
plt.axvline(x=municipal_filtered['FATAL'].mean()*100, color='black', linestyle='--', 
           label=f'M√©dia RMSP: {municipal_filtered["FATAL"].mean()*100:.1f}%')
plt.legend()
plt.gca().invert_yaxis()
plt.tight_layout()
plt.savefig('output/02_pipeline_ml/ranking_municipios_risco.png', dpi=300, bbox_inches='tight')


# PARTE IX: AN√ÅLISE DE INTERA√á√ïES

print("\nAN√ÅLISE DE INTERA√á√ïES ENTRE VARI√ÅVEIS")
print("=" * 60)

# Intera√ß√£o tipo_de_vitima x tipo_via
print("\nINTERA√á√ÉO: TIPO DE V√çTIMA √ó TIPO DE VIA")
interaction_1 = pd.crosstab([df_pred['tipo_de_vitima'], df_pred['tipo_via']], 
                           df_pred['gravidade_lesao'], normalize='index')
print(interaction_1.round(3))

# Intera√ß√£o munic√≠pio x tipo_via para os top 5 munic√≠pios mais perigosos
print("\nINTERA√á√ÉO: TOP 5 MUNIC√çPIOS √ó TIPO DE VIA")
top_5_dangerous = municipal_risk.head(5).index.tolist()
df_top_5 = df_pred[df_pred['municipio'].isin(top_5_dangerous)]

if not df_top_5.empty:
    interaction_2 = pd.crosstab([df_top_5['municipio'], df_top_5['tipo_via']], 
                               df_top_5['gravidade_lesao'], normalize='index')
    print(interaction_2.round(3))

# An√°lise temporal por tipo de v√≠tima
print("\nPADR√ÉO TEMPORAL POR TIPO DE V√çTIMA:")
temporal_victim = df_pred.groupby(['tipo_de_vitima', 'mes_sinistro', 'gravidade_lesao']).size().unstack(fill_value=0)
for victim_type in df_pred['tipo_de_vitima'].unique():
    if victim_type in temporal_victim.index:
        victim_data = temporal_victim.loc[victim_type]
        victim_pct = victim_data.div(victim_data.sum(axis=1), axis=0)
        worst_month = victim_pct['FATAL'].idxmax() if 'FATAL' in victim_pct.columns else 'N/A'
        worst_pct = victim_pct['FATAL'].max() if 'FATAL' in victim_pct.columns else 0
        print(f"‚Ä¢ {victim_type}: Pior m√™s = {worst_month} ({worst_pct*100:.1f}% fatais)")

print("\nPRINCIPAIS DESCOBERTAS ADICIONAIS:")
print("=" * 60)
print(f"‚Ä¢ Padr√£o temporal forte: Dia do m√™s √© o preditor mais importante (28.4%)")
print(f"‚Ä¢ Diferen√ßas geogr√°ficas significativas: Varia√ß√£o de {municipal_safe['FATAL'].iloc[0]*100:.1f}% a {municipal_risk['FATAL'].iloc[0]*100:.1f}%")
print(f"‚Ä¢ Intera√ß√µes complexas entre tipo de v√≠tima e local do acidente")
print(f"‚Ä¢ Sazonalidade mensal pode indicar padr√µes comportamentais espec√≠ficos")

print("\nAN√ÅLISE COMPLETA EXPANDIDA FINALIZADA!")


# TABELA DE COMPARA√á√ÉO FINAL DOS MODELOS


print("\nTABELA DE COMPARA√á√ÉO FINAL (Baseline √ó SMOTE √ó RandomUnderSampler √ó GridSearch √ó GridSearch+SMOTE)")

# Treino GridSearch sem SMOTE
rf_gs_base = RandomForestClassifier(**gs.best_params_, random_state=42, class_weight="balanced")
rf_gs_base.fit(X_train, y_train)
y_pred_gs_base = rf_gs_base.predict(X_test)
f1_gs_base = f1_score(y_test, y_pred_gs_base, average="macro")

# Treino GridSearch + SMOTE (j√° feito)
f1_gs_smote = f1_score(y_test, y_pred_gs, average="macro")

# Tabela resumo
comparacao = pd.DataFrame({
    'Modelo': ['Baseline', 'SMOTE', 'RandomUnderSampler', 'GridSearch', 'GridSearch+SMOTE'],
    'F1-macro': [f1_base, f1_smote, f1_rus, f1_gs_base, f1_gs_smote]
})
print(comparacao.to_string(index=False))

# Salvar tabela em CSV
comparacao.to_csv('output/02_pipeline_ml/comparacao_modelos.csv', index=False, float_format='%.4f', encoding='utf-8')
print("Tabela de compara√ß√£o salva em output/02_pipeline_ml/comparacao_modelos.csv")


# MATRIZ DE DECIS√ÉO (MATRIZ DE CONFUS√ÉO) DOS MODELOS


modelos = {
    'Baseline': y_pred_base,
    'SMOTE': y_pred_smote,
    'RandomUnderSampler': y_pred_rus,
    'GridSearch': y_pred_gs_base,
    'GridSearch+SMOTE': y_pred_gs
}

for nome, y_pred in modelos.items():
    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=grav_encoder_pred.classes_)
    plt.figure(figsize=(7, 6))
    disp.plot(cmap='Blues', values_format='d')
    plt.title(f'Matriz de Decis√£o - {nome}')
    plt.tight_layout()
    plt.savefig(f'output/02_pipeline_ml/matriz_decisao_{nome.lower()}.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"Matriz de decis√£o gerada para {nome}: matriz_decisao_{nome.lower()}.png")
    # Salvar matriz de confus√£o como tabela CSV
    cm_df = pd.DataFrame(cm, index=grav_encoder_pred.classes_, columns=grav_encoder_pred.classes_)
    cm_df.to_csv(f'output/02_pipeline_ml/matriz_confusao_{nome.lower()}.csv', encoding='utf-8')
    print(f"Matriz de confus√£o (tabela) salva para {nome}: matriz_confusao_{nome.lower()}.csv")

    # Compara√ß√£o de m√©tricas dos modelos
    metricas = []
    for nome, y_pred in modelos.items():
        f1 = f1_score(y_test, y_pred, average="macro")
        recall = recall_score(y_test, y_pred, average="macro")
        precision = precision_score(y_test, y_pred, average="macro")
        metricas.append({
            'Modelo': nome,
            'F1-macro': f1,
            'Recall-macro': recall,
            'Precision-macro': precision
        })
    metricas_df = pd.DataFrame(metricas)
    metricas_df.to_csv('output/02_pipeline_ml/metricas_modelos.csv', index=False, float_format='%.4f', encoding='utf-8')
    print("Tabela de m√©tricas dos modelos salva em output/02_pipeline_ml/metricas_modelos.csv")

    # Gr√°fico de barras comparando F1, recall e precis√£o
    plt.figure(figsize=(10, 6))
    bar_width = 0.25
    x = np.arange(len(metricas_df['Modelo']))
    plt.bar(x - bar_width, metricas_df['F1-macro'], width=bar_width, label='F1-macro')
    plt.bar(x, metricas_df['Recall-macro'], width=bar_width, label='Recall-macro')
    plt.bar(x + bar_width, metricas_df['Precision-macro'], width=bar_width, label='Precision-macro')
    plt.xticks(x, metricas_df['Modelo'])
    plt.ylabel('Score')
    plt.title('Compara√ß√£o de M√©tricas dos Modelos')
    plt.legend()
    plt.tight_layout()
    plt.savefig('output/02_pipeline_ml/comparacao_metricas_modelos.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("Gr√°fico de compara√ß√£o de m√©tricas salvo em comparacao_metricas_modelos.png")